{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "**Description**  \n",
    "- In this notebook, I'll be showing my machine learning workflow for creating the sentiment analysis model used by this server\n",
    "- The dataset used is Go-Emotions, a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral\n",
    "- My work was done on kaggle at https://www.kaggle.com/code/leinadest/leinadest-social-media-sentiment-analysis\n",
    "\n",
    "**Summary**\n",
    "1. Ingest the Go-Emotions dataset to provide training and testing data for sentiment analysis\n",
    "2. Initial EDA to see what the data looks like\n",
    "3. Data cleaning\n",
    "4. Feature engineering\n",
    "5. EDA on new features\n",
    "6. More feature engineering\n",
    "7. Experiment with different vectorization methods, classifiers, and hyperparameters\n",
    "8. Train and evaluate the best pipeline\n",
    "9. Save pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import re\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import tarfile\n",
    "import joblib\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Machine learning\n",
    "import mlflow\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Natural-Language Processing\n",
    "import spacy\n",
    "\n",
    "# Cloud\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURATIONS ###\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "\n",
    "### VISUAL HELPERS ###\n",
    "\n",
    "\n",
    "def display_section(title, section_fn):\n",
    "    display(HTML(f'<p style=\"font-size: 1.5rem\">{title}</p>'))\n",
    "    section_fn()\n",
    "    display(HTML(f'<hr/>'))\n",
    "\n",
    "\n",
    "def display_multiple(*dfs):\n",
    "    df_divs = ''.join([f\"<div>{df.to_html()}</div>\" for df in dfs])\n",
    "    display(HTML(f\"<div style='display: flex; gap: 2rem'>{df_divs}</div>\"))\n",
    "\n",
    "\n",
    "### DOWNLOADS\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    return nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "stopwords = get_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_original = pd.concat(\n",
    "    [\n",
    "        pd.read_csv('../data/goemotions_1.csv'),\n",
    "        pd.read_csv('../data/goemotions_2.csv'),\n",
    "        pd.read_csv('../data/goemotions_3.csv'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(ds_original.head())\n",
    "display(ds_original.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA #1\n",
    "\n",
    "**Goal**\n",
    "- Make sense of the Go-Emotions dataset\n",
    "- Decide which sentiments to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [\n",
    "    'admiration',\n",
    "    'amusement',\n",
    "    'anger',\n",
    "    'annoyance',\n",
    "    'approval',\n",
    "    'caring',\n",
    "    'confusion',\n",
    "    'curiosity',\n",
    "    'desire',\n",
    "    'disappointment',\n",
    "    'disapproval',\n",
    "    'disgust',\n",
    "    'embarrassment',\n",
    "    'excitement',\n",
    "    'fear',\n",
    "    'gratitude',\n",
    "    'grief',\n",
    "    'joy',\n",
    "    'love',\n",
    "    'nervousness',\n",
    "    'optimism',\n",
    "    'pride',\n",
    "    'realization',\n",
    "    'relief',\n",
    "    'remorse',\n",
    "    'sadness',\n",
    "    'surprise',\n",
    "    'neutral',\n",
    "]\n",
    "\n",
    "\n",
    "def target_section():\n",
    "    # Display rows with missing label\n",
    "    display(HTML(f'<strong>Missing Labels</strong>'))\n",
    "    ds_unlabeled = ds_original.loc[ds_original[sentiments].sum(axis=1) == 0]\n",
    "    display(ds_unlabeled.head())\n",
    "\n",
    "    # Barplot to visualize distribution of sentiments\n",
    "    ds_original[sentiments].sum().plot(kind='bar')\n",
    "    plt.title('Distribution of Comments Across Sentiments')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Comments')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def text_section():\n",
    "    display(HTML(f'<strong>Duplicate Comments</strong>'))\n",
    "    duplicate_comments = ds_original.loc[ds_original.duplicated(subset=['text'])]\n",
    "    display(duplicate_comments.sort_values(by='text').head())\n",
    "\n",
    "    # Print tokens made from modifications to the comments\n",
    "    tokens = set()\n",
    "    for comment in ds_original['text'].to_list():\n",
    "        token_pattern = r'\\[([A-Z]+)\\]'\n",
    "        token_matches = re.search(token_pattern, comment)\n",
    "        if token_matches:\n",
    "            tokens.add(token_matches[0])\n",
    "    print('Artificial tokens:', tokens)\n",
    "\n",
    "    # Display example comments of each sentiment\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_comments = (\n",
    "            ds_original['text'].loc[ds_original[sentiment] == 1].head().to_list()\n",
    "        )\n",
    "        display(HTML(f'<strong>{sentiment.capitalize()}</strong>'), sentiment_comments)\n",
    "\n",
    "\n",
    "display_section('TARGET', target_section)\n",
    "display_section('TEXT VISUALIZATIONS', text_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Analysis\n",
    "\n",
    "- Examples labeled \"example_very_unclear\" are not labeled with sentiment\n",
    "- There are examples with duplicate comments and have inconsistent sentiment labels\n",
    "- Artificial tokens from data processing: {\\'[T]\\', \\'[ALL]\\', \\'[NAME]\\', \\'[RELIGION]\\'}\n",
    "- Sentiments have a hierarchy such that sentiments 'desire' and 'caring' are extensions of other sentiments such as 'joy', 'sadness', or 'anger'\n",
    "- There are abbreviations, slang, and textual symbols which can be preprocessed\n",
    "- Many sentiments share similarities such as \"anger\" and \"annoyance\"\n",
    "- The frequencies of emotions vary a lot, making this dataset highly imbalanced\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Drop examples labeled \"example_very_unclear\" and examples that are duplicates \n",
    "- Drop examples with labeled 'desire' or 'caring' as I'm interested in more fundamental emotions for my sentiment analysis task\n",
    "- Create a preprocessor that translates abbreviations, slang, and symbols\n",
    "- Group sentiments into five emotional categories: happy, sad, angry, scared, neutral\n",
    "- Undersampling, oversampling, or model-based methods are common approaches to imbalanced datasets\n",
    "    - For simplicity and convenience's sake I'll be using model-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "**Goal**\n",
    "- Drop examples labeled \"example_very_unclear\" as they aren't sentiment-labeled\n",
    "- Drop examples with duplicate comments as they can add bias to the same sentiment and cause confusion with inconsistent sentiments\n",
    "- Group sentiments into more fundamental emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_original.copy()\n",
    "\n",
    "# Drop unclear examples and duplicate comments\n",
    "ds = ds.loc[~ds_original['example_very_unclear']]\n",
    "ds = ds.loc[~ds.duplicated(subset=['text'])]\n",
    "\n",
    "# Drop irrelevant columns\n",
    "ds.drop(\n",
    "    [\n",
    "        'id',\n",
    "        'author',\n",
    "        'subreddit',\n",
    "        'link_id',\n",
    "        'parent_id',\n",
    "        'created_utc',\n",
    "        'rater_id',\n",
    "        'example_very_unclear',\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Drop comments labeled \"caring\" or \"desire\"\n",
    "ds = ds.loc[(ds['caring'] == 0) & (ds['desire'] == 0)]\n",
    "ds.reset_index(drop=True)  # Fix index after dropped rows\n",
    "\n",
    "# Organize sentiments into 6 categories\n",
    "emotions = {\n",
    "    'happy': [\n",
    "        'admiration',\n",
    "        'amusement',\n",
    "        'excitement',\n",
    "        'gratitude',\n",
    "        'joy',\n",
    "        'love',\n",
    "        'optimism',\n",
    "        'pride',\n",
    "        'relief',\n",
    "    ],\n",
    "    'sad': ['sadness', 'grief', 'disappointment', 'remorse'],\n",
    "    'angry': ['anger', 'annoyance', 'disapproval', 'disgust', 'embarrassment'],\n",
    "    'scared': ['fear', 'nervousness'],\n",
    "    'neutral': [\n",
    "        'neutral',\n",
    "        'curiosity',\n",
    "        'approval',\n",
    "        'confusion',\n",
    "        'surprise',\n",
    "        'realization',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Map each sentiment to its emotion category\n",
    "sentiment_mappings = {\n",
    "    sentiment: emotion\n",
    "    for emotion, sentiments in emotions.items()\n",
    "    for sentiment in sentiments\n",
    "}\n",
    "\n",
    "# Compile one-hot encoded emotions into one categorical column\n",
    "emotion_col = ds[sentiments].idxmax(axis=1).map(sentiment_mappings)\n",
    "emotion_col.name = 'emotion'\n",
    "\n",
    "# Replace sentiments with new emotion label\n",
    "ds = pd.concat([ds.drop(sentiments, axis=1), emotion_col], axis=1)\n",
    "\n",
    "\n",
    "# Display results\n",
    "def ds_section():\n",
    "    print('Shape:', ds.shape)\n",
    "    print('Columns:', ds.columns.to_list())\n",
    "    print('Missing values:', ds.isna().sum().sum())\n",
    "    print('Duplicate comments:', ds.duplicated(subset=['text']).sum())\n",
    "\n",
    "\n",
    "def target_section():\n",
    "    distribution_df = ds['emotion'].value_counts().to_frame('Distribution Numbers')\n",
    "    emotion_distribution = ds['emotion'].value_counts()\n",
    "    display(emotion_distribution.to_frame('Distribution Numbers'))\n",
    "    sns.barplot(x=emotion_distribution.index, y=emotion_distribution)\n",
    "    plt.title('Distribution of Target')\n",
    "    plt.xlabel('Emotions')\n",
    "    plt.ylabel('Comments in Sample')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_section('DATASET RESULTS', ds_section)\n",
    "display_section('TARGET RESULTS', target_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['comment_length'] = ds['text'].apply(len)\n",
    "\n",
    "ds['exclamation_mark_count'] = ds['text'].apply(lambda comment: comment.count('!'))\n",
    "\n",
    "ds['question_mark_count'] = ds['text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "ds['period_count'] = ds['text'].apply(lambda comment: comment.count('.'))\n",
    "\n",
    "ds['double_dot_count'] = ds['text'].apply(lambda comment: comment.count('..'))\n",
    "\n",
    "ds['quotation_count'] = ds['text'].apply(lambda comment: comment.count('\"'))\n",
    "\n",
    "\n",
    "def remove_artifacts(comment):\n",
    "    artifacts = {'[T]', '[ALL]', '[NAME]', '[RELIGION]'}\n",
    "    for artifact in artifacts:\n",
    "        comment = comment.replace(artifact, '')\n",
    "    return comment\n",
    "\n",
    "\n",
    "ds['uppercase_count'] = ds['text'].apply(\n",
    "    lambda comment: sum(1 for c in remove_artifacts(comment) if c.isupper())\n",
    ")\n",
    "\n",
    "\n",
    "def count_double_uppercase(comment):\n",
    "    clean_comment = remove_artifacts(comment)\n",
    "    return sum(\n",
    "        1\n",
    "        for i in range(len(clean_comment) - 1)\n",
    "        if clean_comment[i].isupper() and clean_comment[i + 1].isupper()\n",
    "    )\n",
    "\n",
    "\n",
    "ds['double_uppercase_count'] = ds['text'].apply(count_double_uppercase)\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Comment Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['comment_length'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# Histogram to visualize distribution of values\n",
    "sns.histplot(\n",
    "    ds.loc[ds['comment_length'] < 200, 'comment_length'], bins=14, ax=axes[0][0]\n",
    ")\n",
    "axes[0][0].set_title('Distribution of Comment Lengths')\n",
    "axes[0][0].set_xlabel('Comment Length < 200')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Box plot to visualize distribution by emotion\n",
    "sns.boxplot(x='emotion', y='comment_length', data=ds, ax=axes[0][1])\n",
    "axes[0][1].set_title(\"Comment Length Distribution by Emotion\")\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Comment Length')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['comment_length'].mean()\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][0])\n",
    "axes[1][0].set_title('Mean Comment Lengths by Emotion')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Mean Comment Length')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "q1_percents = ds.groupby('emotion')['comment_length'].agg(\n",
    "    lambda group: group.lt(50).mean()\n",
    ")\n",
    "sns.barplot(x=q1_percents.index, y=q1_percents, ax=axes[1][1])\n",
    "axes[1][1].set_title('Percentage of Comments by Emotion in Q1\\n(Comment Length < 50)')\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "iqr_percents = ds.groupby('emotion')['comment_length'].agg(\n",
    "    lambda group: ((group >= 50) & (group <= 100)).mean()\n",
    ")\n",
    "sns.barplot(x=iqr_percents.index, y=iqr_percents, ax=axes[2][0])\n",
    "axes[2][0].set_title(\n",
    "    'Percentage of Comments by Emotion in IQR\\n(Comment Length: [50, 100])'\n",
    ")\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "q4_percents = ds.groupby('emotion')['comment_length'].agg(\n",
    "    lambda group: group.gt(100).mean()\n",
    ")\n",
    "sns.barplot(x=q4_percents.index, y=q4_percents, ax=axes[2][1])\n",
    "axes[2][1].set_title(\n",
    "    'Percentage of Comments by Emotion Beyond Q3\\n(Comment Length > 100)'\n",
    ")\n",
    "axes[2][1].set_xlabel('Emotion')\n",
    "axes[2][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution of comment lengths has a positive skew\n",
    "- Rough approximations: quartile 1 (Q1) between 0 and 50, interquartile (IQR) between 50 and 100, quartile 3 (Q3) to 4 (Q4) between 100 and 200, and a negligible number of outliers past 200\n",
    "- Happy comments show a slight decrease in mean comment length\n",
    "- Comments in Q1:\n",
    "    - Consist of about 0.40 or 40% of happy comments, higher than other emotions\n",
    "    - Consist of about 0.35 or 35% of neutral comments, slightly higher than in angry, sad, and scared comments\n",
    "- Comments in IQR:\n",
    "    - No visible correlation between comment length and emotion\n",
    "- Comments beyond Q3:\n",
    "    - Consist of about 0.18 or 18% of happy comments, lower than other emotions\n",
    "    - Consist of about 0.22 or 22% of neutral comments, slightly lower than in angry, sad, and scared comments\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Comments less than 50 characters are more likely to be happy or neutral, maybe because happy people have less to say and short comments are harder to read emotion from\n",
    "- Comments within 50-100 characters show no correlation with emotion, maybe because most comments are in this range, mixing too many patterns to extract signal from\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- No correlation between comment length and emotions for comments within 50-100 characters\n",
    "- Happy and neutral comments are more likely in comments less than 50 characters and less likely in comments more than 100 characters\n",
    "- Create a binary feature:\n",
    "    - Value: 1 if the comment length is less than 50, otherwise 0\n",
    "    - The hope is that when the\n",
    "        - value is 0, the model will ignore the feature\n",
    "        - value is 1, the model will predict with positive bias toward \"happy\" and \"neutral\"\n",
    "- Create a binary feature:\n",
    "    - Value: 1 if the comment length is more than 100, otherwise 0\n",
    "    - The hope is that when the\n",
    "        - value is 0, the model will ignore the feature\n",
    "        - value is 1, the model will predict with negative bias toward \"happy\" and \"neutral\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['comment_short'] = ds['text'].str.len().lt(50).astype('int8')\n",
    "ds['comment_long'] = ds['text'].str.len().gt(100).astype('int8')\n",
    "\n",
    "display(ds[['comment_length', 'comment_short', 'comment_long']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Exclamation Mark Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['exclamation_mark_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# Bar plot to visualize distribution of values\n",
    "distribution = ds['exclamation_mark_count'].value_counts()\n",
    "sns.barplot(x=distribution.index, y=distribution, color='#0087BD', ax=axes[0][0])\n",
    "axes[0][0].set_title('Distribution of Exclamation Mark Counts')\n",
    "axes[0][0].set_xlabel('Exclamation Mark Count')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['exclamation_mark_count'].mean()\n",
    "sns.barplot(x=means.index, y=means, ax=axes[0][1])\n",
    "axes[0][1].set_title('Mean Exclamation Mark Counts by Emotion')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Mean Exclamation Mark Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['exclamation_mark_count'].agg(\n",
    "    lambda group: group.eq(0).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][0])\n",
    "axes[1][0].set_title('Percentage of Comments by Emotion\\n(Exclamation Mark Count = 0)')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['exclamation_mark_count'].agg(\n",
    "    lambda group: group.eq(1).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][1])\n",
    "axes[1][1].set_title('Percentage of Comments by Emotion\\n(Exclamation Mark Count = 1)')\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['exclamation_mark_count'].agg(\n",
    "    lambda group: group.gt(1).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][0])\n",
    "axes[2][0].set_title('Percentage of Comments by Emotion\\n(Exclamation Mark Count > 1)')\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "axes[2][1].set_visible(False)\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution of exclamation mark counts are heavily positive skewed, with almost 50,000 comments that have 0 exclamation marks and about 5,000 comments with 1\n",
    "- Happy comments contain almost 0.35 mean exclamation marks, more than twice that of other emotions\n",
    "- Comments with 0 exclamation marks:\n",
    "    - Consists of about 0.8 or 80% of happy comments, lower than other emotions\n",
    "- Comments with 1 exclamation mark:\n",
    "    - Consists of about 0.16 or 16% of happy comments, higher than other emotions\n",
    "- Comments with multiple exclamation marks:\n",
    "    - Consists of about 0.06 or 6% of happy comments, higher than other emotions\n",
    "    - Neutral and sad comments decrease in percentage relative to other emotions\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Happy comments have the most exclamation marks on average and are less likely to have 0, maybe because the exclamation mark best expresses happiness than other emotions\n",
    "- As the number of multiple exclamation marks rise, the likelihood for neutral and sad comments decreases, maybe because the use of exclamation marks conflict most with feelings of neutrality or sadness\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Comments with 0 exclamation marks are less likely to be happy\n",
    "- Comments with 1 or more exclamation marks are very likely to be happy\n",
    "- Comments with increasing exclamation marks are increasingly unlikely to be neutral or sad\n",
    "- Current feature \"exclamation_mark_count\": the hope is that when the\n",
    "    - value is 0, the model will predict with negative bias toward \"happy\"\n",
    "    - value is 1, the model with predict with positive bias toward \"happy\"\n",
    "    - value increases, the model will predict with increasing negative bias toward \"neutral\" and \"sad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Question Mark Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['question_mark_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# Bar plot to visualize distribution of values\n",
    "distribution = ds['question_mark_count'].value_counts()\n",
    "sns.barplot(x=distribution.index, y=distribution, ax=axes[0][0], color='#0087BD')\n",
    "axes[0][0].set_title('Distribution of Question Mark Counts')\n",
    "axes[0][0].set_xlabel('Question Mark Count')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['question_mark_count'].mean()\n",
    "sns.barplot(x=means.index, y=means, ax=axes[0][1])\n",
    "axes[0][1].set_title('Mean Question Mark Counts by Emotion')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Mean Question Mark Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['question_mark_count'].agg(\n",
    "    lambda group: group.eq(0).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][0])\n",
    "axes[1][0].set_title('Percentage of Comments by Emotion\\n(Question Mark Count = 0)')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['question_mark_count'].agg(\n",
    "    lambda group: group.eq(1).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][1])\n",
    "axes[1][1].set_title('Percentage of Comments by Emotion\\n(Question Mark Count = 1)')\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['question_mark_count'].agg(\n",
    "    lambda group: group.gt(1).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][0])\n",
    "axes[2][0].set_title('Percentage of Comments by Emotion\\n(Question Mark Count > 1)')\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "axes[2][1].set_visible(False)\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution of question mark counts is heavily positive skewed, with almost 50,000 comments with 0 question marks and about 5,000 with only 1 \n",
    "- Neutral comments have almost 0.190 question marks on average, other comments have on average less than 0.100\n",
    "- Comments with 0 question marks:\n",
    "    - Consist of about 0.8 or 80% of neutral comments, less than other emotions\n",
    "- Comments with 1 question mark:\n",
    "    - Consist of about 0.14 or 14% of neutral comments, higher than all other emotions\n",
    "- Comments with multiple question marks:\n",
    "    - Consist of about 0.0200 or 2% of neutral comments, higher than all other emotions\n",
    "    - Sad and scared comments have risen in percentage relative to angry and happy percentages\n",
    "    - Happy comments have lowered in percentage relative to the rest\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Neutral comments have the most question marks maybe because being inquisitive puts priority on rationality than emotion\n",
    "- As the number of question marks increases, the likelihood the comment is\n",
    "    - sad or scared rises, maybe because more question marks means the comment is more desperate than inquisitive\n",
    "    - happy lowers, maybe because more questioning means less satisfaction and happiness\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Comments with 0 question marks have less chance of being neutral than other emotions\n",
    "- Comments with 1 question mark have a much higher chance of being neutral than other emotions\n",
    "- Comments with multiple question marks have a much higher chance of being neutral, with increased chance of being sad and scared relative to angry and happy chances\n",
    "- Current feature \"question_mark_count\":\n",
    "    - The hope is that when the\n",
    "        - value is 0, the model will predict with a negative bias toward \"neutral\"\n",
    "        - value is 1, the model will predict with a positive bias toward \"neutral\"\n",
    "        - value increases, the model will predict with a positive bias toward \"neutral\", an increasing positive bias toward \"sad\" and \"scared\", and an increasing negative bias toward \"happy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Quotation Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['quotation_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Bar plot to visualize distribution of values\n",
    "distribution = ds['quotation_count'].value_counts()\n",
    "sns.barplot(x=distribution.index, y=distribution, color='#0087BD', ax=axes[0][0])\n",
    "axes[0][0].set_title('Distribution of Quotation Counts')\n",
    "axes[0][0].set_xlabel('Quotation Count')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['quotation_count'].mean()\n",
    "sns.barplot(x=means.index, y=means, ax=axes[0][1])\n",
    "axes[0][1].set_title('Mean Quotation Counts by Emotion')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Mean Quotation Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "quote_percent = ds.groupby('emotion')['quotation_count'].agg(\n",
    "    lambda group: group.eq(2).mean()\n",
    ")\n",
    "sns.barplot(x=quote_percent.index, y=quote_percent, ax=axes[1][0])\n",
    "axes[1][0].set_title(\n",
    "    'Percentage of Comments by Emotion\\n(Quotation Count = 2) ({} Comments)'.format(\n",
    "        ds['quotation_count'].eq(2).sum()\n",
    "    )\n",
    ")\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "quote_percent = ds.groupby('emotion')['quotation_count'].agg(\n",
    "    lambda group: group.eq(4).mean()\n",
    ")\n",
    "sns.barplot(x=quote_percent.index, y=quote_percent, ax=axes[1][1])\n",
    "axes[1][1].set_title(\n",
    "    'Percentage of Comments by Emotion\\n(Quotation Count = 4) ({} Comments)'.format(\n",
    "        ds['quotation_count'].eq(4).sum()\n",
    "    )\n",
    ")\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution is mostly 0 with some in multiples of 2, with over 50,000 comments with 0 double quotes\n",
    "- Comments on average have less double quotes if they are happy, sad, or scared\n",
    "- Comments containing 0 double quotes:\n",
    "    - Consist of about 0.06 or 6% of both angry and neutral comments, more than other emotions\n",
    "- Comments containing 2 double quotes:\n",
    "    - Consist of about 0.020 or 2% of happy comments and about 0.017 or 1.7% of scared comments, less than other emotions\n",
    "- Comments containing 4 double quotes:\n",
    "    - Consist of about 0.0012 or 0.12% of happy and sad comments and 0% of scared comments, much less than other emotions\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Distribution of double quote comes in multiples of 2 likely because double quotes are being used for quotes, each of which need to be wrapped by two double quotes\n",
    "- Angry comments may contain more quotes due to the need to quote those they complain about\n",
    "- Neutral comments may contain more quotes due to explanations, where quotes are useful as persuasive evidence\n",
    "- Happy, sad, and scared comments may contain less quotes because they're focused less on quoting others and more focused toward expressing themselves\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Happy, sad, and scared comments are less likely to have quotes than angry and neutral comments, and this increases as quotation marks increase \n",
    "- Happy and sad comments are further less likely to have two quotes\n",
    "- Current feature \"quotation_count\": the hope is that as the value increases, the model will predict with increasing positive bias toward \"angry\" and \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Period Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['period_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# Histogram to visualize distribution of values\n",
    "sns.histplot(ds['period_count'], bins=20, ax=axes[0][0])\n",
    "axes[0][0].set_title('Distribution of Period Counts')\n",
    "axes[0][0].set_xlabel('Period Count')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Box plot to visualize distribution by emotion\n",
    "sns.boxplot(x='emotion', y='period_count', data=ds, ax=axes[0][1])\n",
    "axes[0][1].set_title(\"Period Count Distribution and Outliers\")\n",
    "axes[0][1].set_xlabel('Period Count')\n",
    "axes[0][1].set_ylabel('Emotion')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['period_count'].mean()\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][0])\n",
    "axes[1][0].set_title('Mean Period Counts by Emotion')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Mean Period Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['period_count'].agg(lambda group: group.lt(2.5).mean())\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][1])\n",
    "axes[1][1].set_title('Percentage of Comments in IQR by Emotion\\n(Period Count < 2.5)')\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['period_count'].agg(\n",
    "    lambda group: (group.ge(2.5) & group.le(5)).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][0])\n",
    "axes[2][0].set_title(\n",
    "    'Percentage of Comments in Q3-4 by Emotion\\n(Period Count: [2.5, 5])'\n",
    ")\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['period_count'].agg(lambda group: group.gt(5).mean())\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][1])\n",
    "axes[2][1].set_title('Percentage of Outlier Comments by Emotion\\n(Period Count > 5)')\n",
    "axes[2][1].set_xlabel('Emotion')\n",
    "axes[2][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show points of most target variation\n",
    "display(\n",
    "    ds.loc[\n",
    "        ds['period_count'].ge(2.5)\n",
    "        & ds['period_count'].le(5)\n",
    "        & (ds['emotion'].eq('scared') | ds['emotion'].eq('sad')),\n",
    "        ['text', 'period_count', 'emotion'],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution is positive skewed, with almost half of the dataset's comments containing no periods\n",
    "- Approximations: quartile 0 to 3 occurs in the range [0, 2.5), quartile 3 to 4 occurs in range \\[2.5, 5.0\\], and outliers occur beyond a period count of 5\n",
    "- On average, happy and neutral comments have less periods\n",
    "- For comments within the IQR:\n",
    "    - 0.9 or 90% of each emotion's comments contain this range of periods; likely no correlation between period count IQR and emotion\n",
    "- For comments within quartiles 3 to 4:\n",
    "    - About 0.09 or 9% of both happy and neutral comments contain this range's period count, lower than other emotions\n",
    "    - About 0.10 or 10% of both angry and sad comments contain this range's period count, lower thaan scared comments\n",
    "- Comments more than 5 periods show some slight variation in emotions\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Happy and neutral comments have the least periods on average, maybe because they're the most light-hearted emotions out of the 5, resulting in more informal language with less use of periods\n",
    "- Comments with less than 3 periods seem to have no correlation with emotion, maybe because there are too many factors behind period count that aren't emotionally driven (ex. abbreviations, sentence endings, personality, etc)\n",
    "- Comments containing between 3 to 5 periods inclusively seem to have a higher chance of being scared or sad, maybe because this number of periods is enough for an double_dot (\"...\"), which can be used as a tool for suspense or sign of weakness\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Comments with less than 3 periods show no correlation between period count as emotion\n",
    "- Comments with 3 to 5 periods have an increased chance of being scared, sad, or angry\n",
    "- Create a feature:\n",
    "    - Values: the number of periods in the comment more than 2\n",
    "    - The hope is that when the\n",
    "        - value is 0, the model will ignore the feature\n",
    "        - value is 1, the model will predict with positive bias toward \"scared\" and negative bias toward \"happy\" and \"neutral\"\n",
    "        - value increases, the model will predict with positive bias toward \"angry\" and negative bias toward \"happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['masked_period_count'] = ds['period_count'].sub(2).clip(0)\n",
    "\n",
    "display(ds.groupby('period_count').agg({'masked_period_count': 'first'}).loc[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Double Dot Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['double_dot_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# Histogram to visualize distribution of values\n",
    "distribution = ds['double_dot_count'].value_counts()\n",
    "sns.barplot(x=distribution.index, y=distribution, color='#0087BD', ax=axes[0][0])\n",
    "axes[0][0].set_title('Distribution of Double Dot Counts')\n",
    "axes[0][0].set_xlabel('Double Dot Count')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['double_dot_count'].mean()\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[0][1])\n",
    "axes[0][1].set_title('Mean Double Dot Counts by Emotion')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Mean Double Dot Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['double_dot_count'].agg(\n",
    "    lambda group: group.eq(0).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][0])\n",
    "axes[1][0].set_title(\n",
    "    'Percentage of Comments by Emotion in Q1\\n(Double Dot Count = 0) (Data Points = {})'.format(\n",
    "        ds['double_dot_count'].eq(0).sum()\n",
    "    )\n",
    ")\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['double_dot_count'].agg(\n",
    "    lambda group: group.eq(1).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[1][1])\n",
    "axes[1][1].set_title(\n",
    "    'Percentage of Comments by Emotion Beyond Q3\\n(Double Dot Count = 1) (Data Points = {})'.format(\n",
    "        ds['double_dot_count'].eq(1).sum()\n",
    "    )\n",
    ")\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['double_dot_count'].agg(\n",
    "    lambda group: group.eq(2).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][0])\n",
    "axes[2][0].set_title(\n",
    "    'Percentage of Comments by Emotion Beyond Q3\\n(Double Dot Count = 2) (Data Points = {})'.format(\n",
    "        ds['double_dot_count'].eq(2).sum()\n",
    "    )\n",
    ")\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "percents = ds.groupby('emotion')['double_dot_count'].agg(\n",
    "    lambda group: group.gt(2).mean()\n",
    ")\n",
    "sns.barplot(x=percents.index, y=percents, ax=axes[2][1])\n",
    "axes[2][1].set_title(\n",
    "    'Percentage of Comments by Emotion Beyond Q3\\n(Double Dot Count > 2) (Data Points = {})'.format(\n",
    "        ds['double_dot_count'].gt(2).sum()\n",
    "    )\n",
    ")\n",
    "axes[2][1].set_xlabel('Emotion')\n",
    "axes[2][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show outliers or points of most target variation\n",
    "display(ds.loc[ds['double_dot_count'].gt(2), ['text', 'double_dot_count', 'emotion']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution has positive skew, with almost all comments containing no double dots and only roughly 5,000 containing any\n",
    "- Happy comments average the lowest of about 0.04 double dots per comment while scared comments average the highest of about 0.10\n",
    "- Percentage of comments with 0 double dots are similar in percentage across all emotions, suggesting no correlation between emotion and having no double dots\n",
    "- Comments with 1 double dot:\n",
    "    - About 0.04 or 4% of happy comments, lowest out of emotions\n",
    "    - About 0.06 or 6% of scared emotions, highest out of emotions\n",
    "- Comments with 2 double dots:\n",
    "    - About 0.010 or 1% of happy comments, lowest out of emotions\n",
    "    - About 0.013 or 1.3% of sad comments, highest out of emotions\n",
    "- Comments with more than 2 double dots:\n",
    "    - About 0.0017 or 0.17% of sad comments, lowest out of emotions\n",
    "    - About 0.0025 or 0.25% of angry comments, highest out of emotions\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Scared comments average the highest number of double dots, maybe because of their use in ellipses for suspense\n",
    "- Happy comments average the least number of double dots, maybe because chained dots usually are accompanied with a lower level of energy, which happiness isn't often paired with\n",
    "- Comments with more than two double dots are most likely angry, maybe because the use of double dots takes on the role of conveying drama or speechlessness\n",
    "- Comments with more than two double dots are least likely sad, maybe because too many double dots is excessively dramatic and is less interesting as a tool of self-expression\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- As the number of a comment's double dots increases, the likelihood of it being happy or sad decreases, and the likelihood of it being scared or angry increases\n",
    "- Current feature \"double_dot_count\": the hope is that when the\n",
    "    - value is 0, the model will ignore the feature\n",
    "    - value is 1, the model will predict with positive bias toward \"scared\" and negative bias toward \"happy\"\n",
    "    - value is 2, the model will predict with negative bias toward \"happy\"\n",
    "    - value > 2, the model will predict with negative bias toward \"happy\" and \"sad\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Uppercase Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['uppercase_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Bar plot to visualize distribution of values\n",
    "close_up = ds.loc[ds['uppercase_count'] < 20, 'uppercase_count']\n",
    "sns.histplot(close_up, bins=20, edgecolor='black', ax=axes[0][0])\n",
    "axes[0][0].set_title('Distribution of Uppercase Counts')\n",
    "axes[0][0].set_xlabel('Uppercase Count < 20')\n",
    "axes[0][0].set_ylabel('Count')\n",
    "\n",
    "# Box plot to visualize distribution by emotion\n",
    "sns.boxplot(x='emotion', y='uppercase_count', data=ds, ax=axes[0][1])\n",
    "axes[0][1].set_title('Uppercase Count Distribution by Emotion\\n(Uppercase Count < 20)')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Uppercase Count')\n",
    "axes[0][1].set_yticks(range(0, 150, 2))\n",
    "axes[0][1].set_ylim(0, 20)\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['uppercase_count'].mean()\n",
    "sns.barplot(x=means.index, y=means, ax=axes[1][0])\n",
    "axes[1][0].set_title('Mean Uppercase Counts by Emotion')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Mean Uppercase Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['uppercase_count'].agg(lambda group: group.le(6).mean())\n",
    "sns.barplot(x=means.index, y=means, ax=axes[1][1])\n",
    "axes[1][1].set_title('Percentage of Comments by Emotion\\n(Uppercase Count <= 6)')\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['uppercase_count'].agg(lambda group: group.gt(6).mean())\n",
    "sns.barplot(x=means.index, y=means)\n",
    "plt.title('Percentage of Comments by Emotion\\n(Uppercase Count > 6)')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Percentage of Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution is positive skewed\n",
    "- Outliers begin after about the value 6\n",
    "- Comments have a mean uppercase count of about 2.50, similar across emotions\n",
    "- Comments with less than or equal to 6 uppercase letters have a mean uppercase count of about 2, similar across emotions\n",
    "- For comments with more than 6 uppercase letters, angry and scared comments on average contain significantly more uppercase letters\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- The majority of comments (where uppercase count <= 6) likely have no correlation between uppercase count and emotion, maybe because there are too many noisy factors for uppercase count shared across emotions (ex. capital letters and comment length, proper nouns, nonspecific emotional high, etc)\n",
    "- Outlier comments (where upper case > 6) have angry comments with significantly more uppercase letters than other comments on average, maybe because some noisy factors have been filtered out by then (ex. it's less likely for a comment to have 6+ sentences and capital letters), and thus more extreme factors like anger or fear persist in contributing to uppercase count\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- When comments contain 6 or less uppercase letters, there's no correlation between uppercase count and emotion\n",
    "- When comments contain more than 6 uppercase letters, angry and scared comments contain more uppercase letters on average\n",
    "- Create a feature:\n",
    "    - Values: the number of uppercase letters over 6 in the comment\n",
    "    - The hope is that when the\n",
    "        - value is 0, the model will ignore the feature\n",
    "        - value increases, the model will predict with increasing positive bias toward \"angry\" and \"scared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['masked_uppercase_count'] = ds['uppercase_count'].sub(6).clip(0)\n",
    "\n",
    "display(ds.groupby('uppercase_count').agg({'masked_uppercase_count': 'first'}).loc[4:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Double Uppercase Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.groupby('emotion')['double_uppercase_count'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Bar plot to visualize distribution of values\n",
    "distribution = ds.loc[\n",
    "    ds['double_uppercase_count'].lt(15), 'double_uppercase_count'\n",
    "].value_counts()\n",
    "sns.barplot(x=distribution.index, y=distribution, color='#0087BD', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Double Uppercase Counts')\n",
    "axes[0].set_xlabel('Double Uppercase Count < 15')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Box plot to visualize distribution\n",
    "sns.boxplot(ds, x='double_uppercase_count', ax=axes[1])\n",
    "axes[1].set_title('Box Plot Distribution of Double Uppercase Count')\n",
    "axes[1].set_xlabel('Double Uppercase Count')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(\n",
    "    ds.loc[\n",
    "        ds['double_uppercase_count'].gt(250),\n",
    "        ['text', 'double_uppercase_count', 'emotion'],\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds = ds.loc[ds['double_uppercase_count'].lt(250)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "display(ds.groupby('emotion')['double_uppercase_count'].describe())\n",
    "\n",
    "# Box plot to visualize distribution\n",
    "distribution = ds.loc[ds['double_uppercase_count'].gt(0)]\n",
    "sns.boxplot(distribution, x='double_uppercase_count', ax=axes[0][0])\n",
    "axes[0][0].set_title(\n",
    "    'Distribution of Double Uppercase Count\\n(Double Uppercase Count > 0)'\n",
    ")\n",
    "axes[0][0].set_xlabel('Double Uppercase Count: [1, 15]')\n",
    "axes[0][0].set_xticks(range(1, 15))\n",
    "axes[0][0].set_xlim(1, 15)\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['double_uppercase_count'].mean()\n",
    "sns.barplot(x=means.index, y=means, ax=axes[0][1])\n",
    "axes[0][1].set_title('Mean Double Uppercase Count by Emotion')\n",
    "axes[0][1].set_xlabel('Emotion')\n",
    "axes[0][1].set_ylabel('Double Uppercase Count')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['double_uppercase_count'].agg(\n",
    "    lambda group: group.eq(0).mean()\n",
    ")\n",
    "sns.barplot(x=means.index, y=means, ax=axes[1][0])\n",
    "axes[1][0].set_title('Percentage of Comments by Emotion\\n(Double Uppercase Count = 0)')\n",
    "axes[1][0].set_xlabel('Emotion')\n",
    "axes[1][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['double_uppercase_count'].agg(\n",
    "    lambda group: group.between(1, 4).mean()\n",
    ")\n",
    "sns.barplot(x=means.index, y=means, ax=axes[1][1])\n",
    "axes[1][1].set_title(\n",
    "    'Percentage of Comments in IQR by Emotion\\n(Double Uppercase Count: [1, 4])'\n",
    ")\n",
    "axes[1][1].set_xlabel('Emotion')\n",
    "axes[1][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['double_uppercase_count'].agg(\n",
    "    lambda group: group.between(5, 8).mean()\n",
    ")\n",
    "sns.barplot(x=means.index, y=means, ax=axes[2][0])\n",
    "axes[2][0].set_title(\n",
    "    'Percentage of Comments in Q3-4 by Emotion\\n(Double Uppercase Count: [5, 8]) (Data Points = {})'.format(\n",
    "        ds['double_uppercase_count'].between(5, 8).sum()\n",
    "    )\n",
    ")\n",
    "axes[2][0].set_xlabel('Emotion')\n",
    "axes[2][0].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Bar plot to visualize correlation with emotion\n",
    "means = ds.groupby('emotion')['double_uppercase_count'].agg(\n",
    "    lambda group: group.gt(8).mean()\n",
    ")\n",
    "sns.barplot(x=means.index, y=means, ax=axes[2][1])\n",
    "axes[2][1].set_title(\n",
    "    'Percentage of Outlier Comments by Emotion\\n(Double Uppercase Count > 8) (Data Points = {})'.format(\n",
    "        ds['double_uppercase_count'].gt(8).sum()\n",
    "    )\n",
    ")\n",
    "axes[2][1].set_xlabel('Emotion')\n",
    "axes[2][1].set_ylabel('Percentage of Comments')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Investigate double uppercase letters in neutral comments\n",
    "display(\n",
    "    ds.loc[\n",
    "        ds['emotion'].eq('neutral') & ds['double_uppercase_count'].gt(0),\n",
    "        ['text', 'double_uppercase_count', 'emotion'],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Distribution is positive skewed with thousands of comments that have at least 1 occurrence of double uppercase letters\n",
    "- There is one major extreme outlier with over 250 double uppercase letters\n",
    "    - This outlier contained over 250 uppercase letters likely for a comical effect, which is why I consider it not correctly neutral and have thus removed it\n",
    "- Distribution of double uppercase count for values over 0:\n",
    "    - Quartile 0 to quartile 3 range between values 1 to 4\n",
    "    - Quartile 3 to 4 range between values 5 to 8\n",
    "    - Outliers occur after 8 double uppercase letters\n",
    "- On average:\n",
    "    - Angry comments contain about 0.7 double uppercase letters, highest percentage\n",
    "    - Sad comments contain about 0.35 double uppercase letters, lowest percentage\n",
    "- Comments with 0 double uppercase letters comprise equal percentage of all emotions; 0 double uppercase letters likely doesn't correlate with emotion\n",
    "- Comments with between 1 to 4 double uppercase letters inclusively:\n",
    "    - Consist of about 0.08 or 8% of neutral comments, highest percentage\n",
    "    - Consist of about 0.06 or 6% of scared comments, lowest percentage\n",
    "- Comments with between 5 to 8 double uppercase letters inclusively:\n",
    "    - Consist of about 0.010 or 1% of angry comments, highest percentage\n",
    "    - Consist of about 0.004 or 0.4% of scared comments, lowest percentage\n",
    "- Comments with over 8 double uppercase letters:\n",
    "    - Consist of about 0.020 or 2% of angry comments, highest percentage\n",
    "    - Consist of about 0.010 or 1% of happy comments, lowest percentage\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- Angry comments have the highest average number of double uppercase letters, maybe because angry yelling is represented by continuous uppercase letters\n",
    "- Happy, sad, and scared comments have the lowest mean number of double uppercase letters, maybe because it's rare for such comments to convey yelling\n",
    "- It's most likely for a comment with 1 to 4 double uppercase letters to be neutral, maybe because those cases often involve an acronym instead of any extreme emotional expression\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- As the number of double uppercase letters increase:\n",
    "    - Angry comments decrease then increase in likelihood\n",
    "    - Happy comments decrease in likelihood\n",
    "    - Neutral comments increase then decrease in likelihood\n",
    "    - Sad comments decrease in likelihood\n",
    "    - Scared comments decrease in likelihood\n",
    "- Current feature \"double_uppercase_count\": the hope is that as the value increases, the model predicts with respect to the pattern above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Pairwise Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_df = pd.get_dummies(ds.drop('text', axis=1)).corr()\n",
    "sns.heatmap(heatmap_df, annot=True, annot_kws=dict(clip_on=True))\n",
    "plt.xlim(12, 17)\n",
    "plt.ylim(0, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- comment_length\n",
    "    - comment_long and comment_short individually have worse correlation scores than comment_length, but added together they correlate more than just comment_length\n",
    "    - comment_long and comment_short have inverse correlations with their corresponding emotions, because they address different stages of comment_length when emotions correlate in a different direction\n",
    "- period_count\n",
    "    - masked_period_count has lower correlation scores in most emotions, because periods and emotions correlation very non-linearly; by capturing more of this pattern, masked_period_count has lower correlation\n",
    "- uppercase_count\n",
    "    - masked_uppercase_count has higher correlation scores in most emotions, because it ignores the range of uppercase letters where there's little to no correlation with emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 n-grams per emotion based on MI\n",
    "top_mi_ngrams = set()\n",
    "top_mi_ngrams_dfs = []\n",
    "\n",
    "\n",
    "def get_top_mi_ngrams():\n",
    "    # N-gram vectorizer for unigrams and bigrams\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "    # N-grams\n",
    "    X = vectorizer.fit_transform(ds['text'])\n",
    "    ngram_names = vectorizer.get_feature_names_out()\n",
    "    ngram_df = pd.DataFrame.sparse.from_spmatrix(X, index=ds.index, columns=ngram_names)\n",
    "\n",
    "    # N-gram frequencies\n",
    "    ngram_counts = ngram_df.sum()\n",
    "    top_1000_ngram_counts = (\n",
    "        ngram_counts.drop(set(stopwords), errors='ignore')\n",
    "        .sort_values(ascending=False)\n",
    "        .head(1000)\n",
    "    )\n",
    "\n",
    "    for emotion in emotions.keys():\n",
    "        # Calculate MI scores for top 1000 n-grams respective to current emotion\n",
    "        top_1000_ngram_names = top_1000_ngram_counts.index\n",
    "        emotion_binary = ds['emotion'] == emotion\n",
    "        mi = mutual_info_classif(\n",
    "            ngram_df[top_1000_ngram_names], emotion_binary, discrete_features=True\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame of n-grams and their MI scores\n",
    "        mi_df = pd.DataFrame(\n",
    "            {'N-gram': top_1000_ngram_names, f'MI for {emotion.capitalize()}': mi}\n",
    "        )\n",
    "\n",
    "        # Sort by MI score and select the 20 most important n-grams for each sentiment\n",
    "        top_20_mi_ngrams = mi_df.sort_values(\n",
    "            by=f'MI for {emotion.capitalize()}', ascending=False\n",
    "        ).head(20)\n",
    "\n",
    "        top_mi_ngrams.update(top_20_mi_ngrams['N-gram'].to_list())\n",
    "        top_mi_ngrams_dfs.append(top_20_mi_ngrams)\n",
    "\n",
    "\n",
    "get_top_mi_ngrams()\n",
    "\n",
    "# Display top MI-scoring n-grams per emotion\n",
    "display_multiple(*top_mi_ngrams_dfs)\n",
    "\n",
    "# Display mean MI scores per emotion's top 20 n-grams\n",
    "means = [df[df.columns[1]].mean() for df in top_mi_ngrams_dfs]\n",
    "sns.barplot(x=['happy', 'sad', 'angry', 'scared', 'neutral'], y=means)\n",
    "plt.title('Mean Scores of Top 20 MI-Scoring N-Grams by Emotion')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Mean MI Score')\n",
    "plt.show()\n",
    "\n",
    "# Display all top 20 MI-scoring n-grams\n",
    "print('Top Important N-Grams (Union Across Emotions):', top_mi_ngrams, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Going by average scores for each emotion's top MI-scoring n-grams, the most to least explainable emotions:\n",
    "    1. happy\n",
    "    2. neutral\n",
    "    3. angry\n",
    "    4. sad\n",
    "    5. scared\n",
    " \n",
    "### Conclusion\n",
    "\n",
    "- Using the n-grams, a model will predict some emotions with higher precision than others\n",
    "- Include each top MI-scoring n-gram as a binary feature\n",
    "    - Value: 1 if the n-gram is in the comment, 0 otherwise\n",
    "    - The hope is that when the\n",
    "        -  value is 0, the model will ignore the feautre\n",
    "        -  value is 1, the model will predict with positive bias toward whichever emotion the n-gram is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_ngrams = {\n",
    "    'nice',\n",
    "    'cringe',\n",
    "    'awful',\n",
    "    'watching',\n",
    "    'great',\n",
    "    'sad',\n",
    "    'worst',\n",
    "    'stop',\n",
    "    'thanks',\n",
    "    'you for',\n",
    "    'horrible',\n",
    "    'scared',\n",
    "    'about it',\n",
    "    'worry',\n",
    "    'haha',\n",
    "    'unfortunately',\n",
    "    'sorry for',\n",
    "    'terrible',\n",
    "    'lol',\n",
    "    'hope',\n",
    "    'shit',\n",
    "    'the fuck',\n",
    "    'glad',\n",
    "    'thank',\n",
    "    'love',\n",
    "    'miss',\n",
    "    'crying',\n",
    "    'missed',\n",
    "    'sorry',\n",
    "    'amazing',\n",
    "    'so sorry',\n",
    "    'appreciate',\n",
    "    'imagine',\n",
    "    'lost',\n",
    "    'funny',\n",
    "    'cool',\n",
    "    'weird',\n",
    "    'stupid',\n",
    "    'love this',\n",
    "    'hate',\n",
    "    'fuck',\n",
    "    'happy',\n",
    "    'fucking',\n",
    "    'good',\n",
    "    'afraid',\n",
    "    'bad',\n",
    "    'thank you',\n",
    "    'awesome',\n",
    "    'the worst',\n",
    "    'die',\n",
    "    'hurt',\n",
    "    'love it',\n",
    "    'thanks for',\n",
    "    'poor',\n",
    "    'feel',\n",
    "    'disgusting',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns for handling specific Reddit content\n",
    "URL_PATTERN = r'http[s]?://\\S+|www\\.\\S+'\n",
    "MENTION_PATTERN = r'@\\w+'\n",
    "HASHTAG_PATTERN = r'#\\w+'\n",
    "\n",
    "# Slang translations\n",
    "slang_dict = {\n",
    "    'lol': 'amused',\n",
    "    'lmao': 'hilarious',\n",
    "    'rofl': 'rolling_on_floor_laughing',\n",
    "    'omg': 'surprised',\n",
    "    'brb': 'temporarily_away',\n",
    "    'wtf': 'confused',\n",
    "    'fml': 'frustrated',\n",
    "    'smh': 'disbelief',\n",
    "    'omfg': 'extremely_surprised',\n",
    "    'idk': 'unsure',\n",
    "    'tbh': 'honestly',\n",
    "    'sus': 'suspicious',\n",
    "    'bruh': 'exasperated',\n",
    "    'noob': 'inexperienced',\n",
    "    'simp': 'overly_adoring',\n",
    "    'tldr': 'summary',\n",
    "    'fomo': 'fear_of_missing_out',\n",
    "    'yta': 'you_are_the_asshole',\n",
    "    'nta': 'not_the_asshole',\n",
    "    'nvm': 'forget_it',\n",
    "    'gg': 'good_game',\n",
    "    'cya': 'goodbye',\n",
    "    'lmfao': 'laughing_my_ass_off',\n",
    "    'tifu': 'today_i_fucked_up',\n",
    "    'wbu': 'what_about_you',\n",
    "    'ty': 'thank_you',\n",
    "    'yw': 'you_are_welcome',\n",
    "    'iama': 'i_am_a',\n",
    "    'eli5': 'explain_like_im_5',\n",
    "    'ftw': 'for_the_win',\n",
    "    'fyi': 'for_your_information',\n",
    "    'np': 'no_problem',\n",
    "    'wb': 'welcome_back',\n",
    "    'lit': 'exciting',\n",
    "    'fam': 'friend',\n",
    "    'stan': 'extremely_support',\n",
    "    'savage': 'bold_and_ruthless',\n",
    "    'cap': 'lie_or_false',\n",
    "    'no cap': 'honestly',\n",
    "    'lmk': 'let_me_know',\n",
    "    'sksksk': 'excited_or_sentimental_laughter',\n",
    "    'stfu': 'shut_up',\n",
    "    'lowkey': 'slightly',\n",
    "    'highkey': 'very',\n",
    "    'yeet': 'throw_or_discard',\n",
    "    'af': 'as_fuck',\n",
    "    'deadass': 'seriously',\n",
    "    'ppl': 'people',\n",
    "    'imo': 'in_my_opinion',\n",
    "    'wdym': 'what_do_you_mean',\n",
    "    'kys': 'kill_yourself',\n",
    "    'kms': 'kill_myself',\n",
    "}\n",
    "\n",
    "# Symbol translations\n",
    "symbol_dict = {\n",
    "    ':)': 'happy',\n",
    "    ':-)': 'happy',\n",
    "    ':D': 'happy',\n",
    "    ':-D': 'happy',\n",
    "    ':P': 'happy',\n",
    "    ':-P': 'happy',\n",
    "    ':]': 'happy',\n",
    "    ':-]': 'happy',\n",
    "    ':3': 'happy',\n",
    "    ':^)': 'happy',\n",
    "    ':(': 'sad',\n",
    "    ':-(': 'sad',\n",
    "    'D:': 'sad',\n",
    "    ':-C': 'sad',\n",
    "    'T_T': 'sad',\n",
    "    \":'(\": 'sad',\n",
    "    ':|': 'neutral',\n",
    "    ':o': 'neutral',\n",
    "    ':-o': 'neutral',\n",
    "    'O:': 'neutral',\n",
    "    'O:-': 'neutral',\n",
    "    '^_^': 'happy',\n",
    "    '>:(': 'angry',\n",
    "    '>:-(': 'angry',\n",
    "    ':X': 'angry',\n",
    "    ':@': 'angry',\n",
    "    ':-|': 'neutral',\n",
    "    ':/': 'neutral',\n",
    "    ':\\\\': 'neutral',\n",
    "    ':-/': 'neutral',\n",
    "    ';-)': 'playful',\n",
    "    ';P': 'playful',\n",
    "    ':*': 'love',\n",
    "    '<3': 'love',\n",
    "    '/s': 'satire',\n",
    "    '/jk': 'just_kidding',\n",
    "}\n",
    "\n",
    "# Emoji translations\n",
    "emoji_dict = {\n",
    "    '': 'sad',\n",
    "    '': 'happy',\n",
    "    '': 'love',\n",
    "    '': 'laugh',\n",
    "    '': 'angry',\n",
    "    '': 'scared',\n",
    "    '': 'sad',\n",
    "    '': 'sad',\n",
    "    '': 'sad',\n",
    "    '': 'playful',\n",
    "    '': 'cool',\n",
    "    '': 'angry',\n",
    "    '': 'happy',\n",
    "    '': 'neutral',\n",
    "    '': 'happy',\n",
    "    '': 'sad',\n",
    "    '': 'neutral',\n",
    "    '': 'love',\n",
    "    '': 'love',\n",
    "    '': 'playful',\n",
    "    '': 'sad',\n",
    "    '': 'embarrassed',\n",
    "    '': 'happy',\n",
    "    '': 'scared',\n",
    "    '': 'happy',\n",
    "    '': 'angry',\n",
    "    '': 'laugh',\n",
    "    '': 'happy',\n",
    "    '': 'happy',\n",
    "    '': 'happy',\n",
    "}\n",
    "\n",
    "# Stopwords to retain as they contribute sentiment information\n",
    "stopword_exceptions = {\n",
    "    'not',\n",
    "    'very',\n",
    "    'really',\n",
    "    'too',\n",
    "    'just',\n",
    "    'even',\n",
    "    'so',\n",
    "    'still',\n",
    "    'quite',\n",
    "    'always',\n",
    "    'never',\n",
    "    'sometimes',\n",
    "    'more',\n",
    "    'less',\n",
    "    'almost',\n",
    "    'kinda',\n",
    "    'sorta',\n",
    "    'meh',\n",
    "    'yet',\n",
    "    'really',\n",
    "    'always',\n",
    "}\n",
    "\n",
    "# Punctuation to retain as they contribute sentiment information\n",
    "punctuation_exceptions = {'!', '?', '.'}\n",
    "\n",
    "### PREPROCESSORS ###\n",
    "\n",
    "\n",
    "class BasePreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "class RedditTextCleaner(BasePreprocessor):\n",
    "    '''An sklearn transformer that cleans reddit comments.'''\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Preprocess a pandas series of strings.'''\n",
    "        X_copy = pd.DataFrame(X)\n",
    "        X_copy['text'] = X['text'].apply(self._preprocess_reddit_comment)\n",
    "        return X_copy\n",
    "\n",
    "    def _preprocess_reddit_comment(self, text):\n",
    "        '''Custom tokenizer for Reddit comments'''\n",
    "        text = re.sub(URL_PATTERN, '<URL>', text)\n",
    "        text = re.sub(MENTION_PATTERN, '<USER>', text)\n",
    "        text = re.sub(HASHTAG_PATTERN, '<HASHTAG>', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "class Tokenizer(BasePreprocessor):\n",
    "    '''An sklearn transformer that tokenizes and processes text.'''\n",
    "\n",
    "    def __init__(self, exceptions=None):\n",
    "        self.exceptions = exceptions or stopword_exceptions.union(\n",
    "            punctuation_exceptions\n",
    "        )\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Tokenize a pandas series of strings.'''\n",
    "        print('Tokenizer...')\n",
    "        nlp = spacy.load('en_core_web_lg', disable=['ner', 'parser', 'tok2vec'])\n",
    "        token_docs = nlp.pipe(X['text'], batch_size=1000)\n",
    "        X['tokens'] = [self._process_tokens(doc) for doc in token_docs]\n",
    "        return X\n",
    "\n",
    "    def _process_tokens(self, tokens):\n",
    "        '''Translate slang and symbols and filter punctuation and stopwords.'''\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.lower_ in self.exceptions:\n",
    "                processed_tokens.append(token.lower_)\n",
    "            elif token.lower_ in slang_dict:\n",
    "                processed_tokens.append(slang_dict.get(token.lower_))\n",
    "            elif token.text in emoji_dict:\n",
    "                processed_tokens.append(emoji_dict.get(token.text))\n",
    "            elif not token.is_stop and token.is_alpha:\n",
    "                processed_tokens.append(token.lemma_.lower())\n",
    "        return processed_tokens\n",
    "\n",
    "\n",
    "class FeatureEngineer(BasePreprocessor):\n",
    "    '''An sklearn transformer that engineers features.'''\n",
    "\n",
    "    def __init__(self, ngrams):\n",
    "        self.ngrams = ngrams\n",
    "\n",
    "    def transform(self, X):\n",
    "        print('FeatureEngineer...')\n",
    "        X['comment_short'] = X['text'].apply(len).lt(50).astype('int8')\n",
    "        X['comment_long'] = X['text'].apply(len).gt(100).astype('int8')\n",
    "        X['exclamation_mark_count'] = X['text'].str.count('!')\n",
    "        X['question_mark_count'] = X['text'].str.count('\\?')\n",
    "        X['masked_period_count'] = X['text'].str.count('.').sub(2).clip(0)\n",
    "        X['double_dot_count'] = X['text'].str.count('..')\n",
    "        X['quotation_count'] = X['text'].str.count('\"\"')\n",
    "        X['masked_uppercase_count'] = (\n",
    "            X['text']\n",
    "            .apply(\n",
    "                lambda comment: sum(\n",
    "                    1 for c in self._remove_artifacts(comment) if c.isupper()\n",
    "                )\n",
    "            )\n",
    "            .sub(6)\n",
    "            .clip(0)\n",
    "        )\n",
    "        X['double_uppercase_count'] = X['text'].apply(self._count_double_uppercase)\n",
    "        X = self._add_ngrams(X)\n",
    "        return X\n",
    "\n",
    "    def _remove_artifacts(self, comment):\n",
    "        artifacts = {\n",
    "            '[T]',\n",
    "            '[ALL]',\n",
    "            '[NAME]',\n",
    "            '[RELIGION]',\n",
    "            '<URL>',\n",
    "            '<USER>',\n",
    "            '<HASHTAG>',\n",
    "        }\n",
    "        for artifact in artifacts:\n",
    "            comment = comment.replace(artifact, '')\n",
    "        return comment\n",
    "\n",
    "    def _count_double_uppercase(self, comment):\n",
    "        clean_comment = self._remove_artifacts(comment)\n",
    "        return sum(\n",
    "            1\n",
    "            for i in range(len(clean_comment) - 1)\n",
    "            if clean_comment[i].isupper() and clean_comment[i + 1].isupper()\n",
    "        )\n",
    "\n",
    "    def _add_ngrams(self, X):\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2), vocabulary=self.ngrams)\n",
    "        ngrams_csrm = vectorizer.fit_transform(X['text'])\n",
    "        ngram_names = ['ngram_' + ngram for ngram in vectorizer.get_feature_names_out()]\n",
    "        ngrams_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "            ngrams_csrm, index=X.index, columns=ngram_names\n",
    "        )\n",
    "        return pd.concat([X, ngrams_df], axis=1)\n",
    "\n",
    "\n",
    "class Scaler(BasePreprocessor):\n",
    "    '''An sklearn transformer that scales input for machine learning.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.unscaled_features = [\n",
    "            'masked_period_count',\n",
    "            'double_dot_count',\n",
    "            'quotation_count',\n",
    "            'exclamation_mark_count',\n",
    "            'question_mark_count',\n",
    "            'masked_uppercase_count',\n",
    "            'double_uppercase_count',\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.unscaled_features])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print('Scaler...')\n",
    "        scaled_data = self.scaler.transform(X[self.unscaled_features])\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaled_data, index=X.index, columns=self.unscaled_features\n",
    "        )\n",
    "        X_scaled = pd.concat([X.drop(self.unscaled_features, axis=1), X_scaled], axis=1)\n",
    "        return X_scaled\n",
    "\n",
    "\n",
    "class Cleaner(BasePreprocessor):\n",
    "    '''An sklearn transformer that cleans up input for machine learning.'''\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Drop temporary features and compress features.'''\n",
    "        print('Cleaner...')\n",
    "        X_clean = X.drop(\n",
    "            ['text', 'tokens', 'processed_tokens'], axis=1, errors='ignore'\n",
    "        )\n",
    "        X_clean.columns = X_clean.columns.astype(str)\n",
    "        X_csrm = csr_matrix(X_clean)\n",
    "        return X_csrm\n",
    "\n",
    "\n",
    "### EXAMPLE ###\n",
    "\n",
    "example_pipeline = Pipeline(\n",
    "    [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer(stopword_exceptions.update({'/s', ':-)'}))),\n",
    "        ('feature_engineer', FeatureEngineer(important_ngrams)),\n",
    "        ('scaler', Scaler()),\n",
    "        # ('feature_cleaner', Cleaner())\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_corpus = pd.DataFrame(\n",
    "    {'text': ['www.website.com @user #hashtag LOL NOT A ! ? . | /s \"\"\"\":-)']}\n",
    ")\n",
    "\n",
    "preprocessed_example = example_pipeline.fit_transform(example_corpus)\n",
    "preprocessed_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "\n",
    "**Evaluation Metrics (Ordered by Priority)**\n",
    "1. F1-score (measures balance of recall and precision)\n",
    "2. Recall (measures how many correct predictions are made for each class)\n",
    "3. Precision (measures how carefully predictions are made for each class)\n",
    "4. Accuracy (measures how well the model fitted to the training set)\n",
    "\n",
    "**Strategy**\n",
    "1. Experiment with two pipeline strategies for traditional sentiment analysis: TF-IDF vectorization and embedding\n",
    "    - TF-IDF vectorization\n",
    "        - Transforms a corpus into a document term matrix, where each term's value represents the term's importance by how frequent it is in the same text in constrast with its frequency across the dataset\n",
    "        - Each term's value tells the classifier how much importance the term carries in the comment\n",
    "        - The hope is that sentimentally-similar comments place similar importance on similar terms and that the model can fit to this pattern\n",
    "    - Embedding\n",
    "        - Transforms each token into a vector whose every component carries the token's semantic meaning in some dimension\n",
    "        - Embeddings in a comment can be averaged to tell the classifier the overall semantic meaning of the comment\n",
    "        - The hope is that semantically-similar comments will be sentimentally-similar and that the model can fit to this pattern\n",
    "2. Experiment with different classifiers on the best pipeline\n",
    "3. Experiment with hyperparameter tuning on the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURATIONS ###\n",
    "\n",
    "TRACKING_URI = os.environ['TRACKING_URI']\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "skip_experiments = False  # Toggle to skip/run experiments\n",
    "\n",
    "### EXPERIMENT DATA STRUCTURE ###\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, name, X, y):\n",
    "        if not skip_experiments:\n",
    "            mlflow.set_experiment(name)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def _report_evaluation(self, name, y_pred, time):\n",
    "        # Evaluate\n",
    "        report = classification_report(\n",
    "            self.y,\n",
    "            y_pred,\n",
    "            target_names=['happy', 'sad', 'angry', 'scared', 'neutral'],\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "\n",
    "        # Log individual metrics for each class\n",
    "        for class_idx, class_name in enumerate(report.keys()):\n",
    "            avgs = ['accuracy', 'macro avg', 'weighted avg']\n",
    "            if class_name not in avgs:\n",
    "                mlflow.log_metric(\n",
    "                    f'precision_class_{class_idx}', report[class_name]['precision']\n",
    "                )\n",
    "                mlflow.log_metric(\n",
    "                    f'recall_class_{class_idx}', report[class_name]['recall']\n",
    "                )\n",
    "                mlflow.log_metric(\n",
    "                    f'f1_class_{class_idx}', report[class_name]['f1-score']\n",
    "                )\n",
    "\n",
    "        # Log overall accuracy and averages\n",
    "        mlflow.log_metric('accuracy', report['accuracy'])\n",
    "        mlflow.log_metric('precision_macro', report['macro avg']['precision'])\n",
    "        mlflow.log_metric('recall_macro', report['macro avg']['recall'])\n",
    "        mlflow.log_metric('f1_macro', report['macro avg']['f1-score'])\n",
    "\n",
    "        # Print results\n",
    "        print('Ran pipeline', name)\n",
    "        print('Accuracy:', report['accuracy'])\n",
    "        print('Precision macro:', report['macro avg']['precision'])\n",
    "        print('Recall macro:', report['macro avg']['recall'])\n",
    "        print('F1 macro:', report['macro avg']['f1-score'])\n",
    "        print('Time {:.2f} seconds'.format(time))\n",
    "\n",
    "    def run_evaluation(self, name, pipeline, fit_params={}):\n",
    "        if skip_experiments:\n",
    "            return\n",
    "\n",
    "        classifier = pipeline.named_steps['classifier']\n",
    "\n",
    "        full_params = [\n",
    "            ('preprocessing_{}_{}'.format(i, name), step)\n",
    "            for (i, (name, step)) in enumerate(pipeline.named_steps.items())\n",
    "        ] + [\n",
    "            ('hyperparameter_{}'.format(name), hyperparameter)\n",
    "            for name, hyperparameter in classifier.get_params().items()\n",
    "        ]\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            for param_name, param in full_params:\n",
    "                mlflow.log_param(param_name, param)\n",
    "\n",
    "            start_time = time()\n",
    "\n",
    "            print(\n",
    "                'Pipeline {}: cross-validation fitting and predicting...'.format(name),\n",
    "                flush=True,\n",
    "            )\n",
    "            y_pred = cross_val_predict(pipeline, self.X, self.y, fit_params=fit_params)\n",
    "\n",
    "            end_time = time()\n",
    "\n",
    "            self._report_evaluation(name, y_pred, end_time - start_time)\n",
    "\n",
    "    def run_tuning(self, name, random_search):\n",
    "        if skip_experiments:\n",
    "            return\n",
    "\n",
    "        full_params = [('model', random_search.estimator)]\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            for param_name, param in full_params:\n",
    "                mlflow.log_param(param_name, param)\n",
    "\n",
    "            start_time = time()\n",
    "\n",
    "            print('Pipeline {}: tuning...'.format(name), flush=True)\n",
    "            random_search.fit(self.X, self.y)\n",
    "\n",
    "            print(\n",
    "                'Pipeline {}: cross-validation fitting and predicting...'.format(name),\n",
    "                flush=True,\n",
    "            )\n",
    "            y_pred = cross_val_predict(random_search.best_estimator_, self.X, self.y)\n",
    "\n",
    "            end_time = time()\n",
    "\n",
    "            mlflow.log_params(random_search.best_params_)\n",
    "            self._report_evaluation(name, y_pred, end_time - start_time)\n",
    "            print('Best Hyperparameters:', random_search.best_params_)\n",
    "\n",
    "        return random_search\n",
    "\n",
    "\n",
    "### DATA SPLIT ###\n",
    "\n",
    "sample_size = 0.2  # About 11,000 examples\n",
    "sample, _ = train_test_split(\n",
    "    ds, train_size=sample_size, random_state=0, stratify=ds['emotion']\n",
    ")\n",
    "\n",
    "X_sample = sample[['text']]\n",
    "y_sample = sample['emotion'].map(\n",
    "    {'happy': 0, 'sad': 1, 'angry': 2, 'scared': 3, 'neutral': 4}\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF vs. Embeddings Experiment\n",
    "\n",
    "**Goal**: Determine whether TF-IDF vectors or embeddings are superior predictive features for sentiment analysis on reddit comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF and Embedding Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF ###\n",
    "\n",
    "\n",
    "class TfidfTransformer(BasePreprocessor):\n",
    "    '''A sklearn transformer that transforms X[\"tokens\"] to TF-IDF vectors.'''\n",
    "\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            preprocessor=self.keep_tokens,\n",
    "            tokenizer=self.keep_tokens,\n",
    "            token_pattern=None,\n",
    "            vocabulary=self.vocabulary,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def keep_tokens(tokens):\n",
    "        return tokens\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tfidf_vectorizer.fit(X['tokens'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        tfidf_matrix = self.tfidf_vectorizer.transform(X['tokens'])\n",
    "        tokens = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        tokens = ['tfidf_' + token for token in tokens]\n",
    "        tfidf_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "            tfidf_matrix, columns=tokens, index=X.index\n",
    "        )\n",
    "        return pd.concat([X, tfidf_df], axis=1)\n",
    "\n",
    "\n",
    "### EMBEDDING ###\n",
    "\n",
    "\n",
    "class Embedder(BasePreprocessor):\n",
    "    '''An sklearn transformer that transforms X[\"text\"] to embeddings.'''\n",
    "\n",
    "    def __init__(self, embeddings_path, embedding_dim):\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings_index = dict()\n",
    "        self.embeddings_set = set()\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._load_embeddings()\n",
    "        embeddings = X['text'].apply(lambda x: self._get_average_embedding(x))\n",
    "        embeddings_matrix = np.vstack(embeddings.values)\n",
    "        embeddings_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "            csr_matrix(embeddings_matrix),\n",
    "            index=X.index,\n",
    "        )\n",
    "        return pd.concat([X, embeddings_df], axis=1)\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        with open(self.embeddings_path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                self.embeddings_index[word] = coefs\n",
    "                self.embeddings_set.add(word)\n",
    "\n",
    "    def _get_average_embedding(self, tokens):\n",
    "        # Initialize a vector of zeros\n",
    "        embedding_vector = np.zeros(self.embedding_dim).copy()\n",
    "        valid_token_count = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.embeddings_set:\n",
    "                embedding_vector += self.embeddings_index[token]\n",
    "                valid_token_count += 1\n",
    "\n",
    "        # If no valid tokens, return a zero vector\n",
    "        if valid_token_count > 0:\n",
    "            embedding_vector /= valid_token_count\n",
    "\n",
    "        return embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### HELPERS ###\n",
    "\n",
    "# Tokens to retain during Tokenizer filtering\n",
    "special_tokens = list(\n",
    "    stopword_exceptions.union(\n",
    "        punctuation_exceptions, slang_dict.keys(), symbol_dict.keys(), emoji_dict.keys()\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def get_ds_vocabulary():\n",
    "    ds_clean = RedditTextCleaner().transform(ds[['text']])\n",
    "    ds_tokenized = Tokenizer(special_tokens).transform(ds_clean)\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        preprocessor=lambda tokens: tokens,\n",
    "        tokenizer=lambda tokens: tokens,\n",
    "        token_pattern=None,\n",
    "    )\n",
    "    tfidf_vectorizer.fit(ds_tokenized['tokens'])\n",
    "    vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "ds_vocabulary = get_ds_vocabulary()\n",
    "\n",
    "\n",
    "def create_tfidf_pipeline(classifier=None, scale=True):\n",
    "    steps = [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer(special_tokens)),\n",
    "        ('feature_engineer', FeatureEngineer(important_ngrams)),\n",
    "        ('tfidf_vectorizer', TfidfTransformer(ds_vocabulary)),\n",
    "    ]\n",
    "    if scale:\n",
    "        steps.append(('scaler', Scaler()))\n",
    "    steps.append(('feature_cleaner', Cleaner()))\n",
    "    if classifier is not None:\n",
    "        steps.append(('classifier', classifier))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def create_embeddings_pipeline(classifier=None):\n",
    "    embeddings_path = '/kaggle/input/glove6b300dtxt/glove.6B.300d.txt'\n",
    "    steps = [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer()),\n",
    "        ('feature_engineer', FeatureEngineer(important_ngrams)),\n",
    "        ('embedder', Embedder(embeddings_path, 300)),\n",
    "        ('scaler', Scaler()),\n",
    "        ('feature_cleaner', Cleaner()),\n",
    "    ]\n",
    "    if classifier is not None:\n",
    "        steps.append(('classifier', classifier))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "### EXPERIMENT ###\n",
    "\n",
    "pipelines = [\n",
    "    {\n",
    "        'name': 'tfidf',\n",
    "        'pipeline': create_tfidf_pipeline(\n",
    "            RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        'name': 'embedding',\n",
    "        'pipeline': create_embeddings_pipeline(\n",
    "            RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "experiment = Experiment('TF-IDF_vs_embedding', X_train, y_train)\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    experiment.run_evaluation(**pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Pipeline tfidf performed with higher accuracy (more predictions correct), higher precision macro (more positive predictions correct on average), recall macro (higher percentage of class examples predicted correct on average), and f1 macro (classification performance score factoring in precision and recall).\n",
    "\n",
    "### Discussion  \n",
    "\n",
    "I've heard that models typically perform better on sentiment analysis using embeddings over TF-IDF vectors, however in this case the model performed better by TF-IDF vectorzation. The reason, I believe, why TF-IDF vectors are more helpful to the model: comments on reddit are often mixed with sarcasm, satire, jokes, slang, and other informal uses of language that are better captured by token behavior rather than token meanings. \n",
    "\n",
    "TF-IDF captures token behavior by factoring in a token's frequency in its comment and across the dataset to determine its importance to its respective comment. By doing so, the features will have essentially captured the behavior of the tokens in each comment. \n",
    "\n",
    "Embeddings capture token definition by vectorizing each token into an embedding, which is a vector that represents the token's semantics, such that semantically-similar tokens have nearby embeddings and semantically-different tokens have faraway embeddings. The embeddings preprocessor averages each comment's embeddings in order to transform each comment into a single embedding that represents the meaning of the entire comment.\n",
    "\n",
    "The problem with relying on pretrained embeddings from GloVe for vectorizing reddit comments is that the embeddings weren't trained on reddit comments and thus can sometimes misrepresent their corresponding tokens. TF-IDF values don't face the same problem because they don't care about what each token means but instead how it was used.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Use TF-IDF vectors instead of embeddings for predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Experiment\n",
    "\n",
    "**Goal**: Determine which classifier performs best using the pipeline with TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipelines = [\n",
    "    {\n",
    "        'name': 'random_forest',\n",
    "        'pipeline': create_tfidf_pipeline(\n",
    "            RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        'name': 'logistic_regression',\n",
    "        'pipeline': create_tfidf_pipeline(\n",
    "            LogisticRegression(class_weight='balanced', random_state=0),\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        'name': 'svm',\n",
    "        'pipeline': create_tfidf_pipeline(\n",
    "            SVC(kernel='linear', class_weight='balanced', random_state=0),\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        'name': 'naive_bayes',\n",
    "        'pipeline': create_tfidf_pipeline(\n",
    "            MultinomialNB(class_prior=[1, 1, 1, 1, 1]), scale=False\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "experiment = Experiment('best_classifier', X_sample, y_sample)\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    experiment.run_evaluation(**pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Random Forest has the highest accuracy and precision, but it suffers from high recall in majority classes and terrible recall in minority classes.\n",
    "\n",
    "I've decided to use SVM because it has the second highest accuracy and the highest f1 score, which takes into account recall and precision and scores the model for its consistent performance across classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "**Strategy**\n",
    "- Embedded approach to feature selection using Random Forest's feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "fs_pipeline = create_tfidf_pipeline(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=0),\n",
    ")\n",
    "\n",
    "fs_pipeline.fit(X_sample, y_sample)\n",
    "\n",
    "# Get feature importances from feature coefficients\n",
    "importances = fs_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "features = fs_pipeline[:-2].fit_transform(X_sample.head(1)).columns[2:]\n",
    "\n",
    "# Create table of features and their importances\n",
    "features_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "features_df = features_df.loc[features_df['Importance'].gt(0)]\n",
    "features_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "features_df.to_csv('feature_importances.csv')\n",
    "\n",
    "# Select TF-IDF and n-gram features\n",
    "selected_tfidf = (\n",
    "    features_df['Feature']\n",
    "    .loc[features_df['Feature'].str.startswith('tfidf_')]\n",
    "    .apply(lambda x: x.replace('tfidf_', ''))\n",
    "    .to_list()\n",
    ")\n",
    "selected_ngrams = (\n",
    "    features_df['Feature']\n",
    "    .loc[features_df['Feature'].str.startswith('ngram_')]\n",
    "    .apply(lambda x: x.replace('ngram_', ''))\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(selected_tfidf)\n",
    "print(selected_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fs_test_pipeline = Pipeline(\n",
    "    [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer(special_tokens)),\n",
    "        ('feature_engineer', FeatureEngineer(selected_ngrams)),\n",
    "        ('tfidf_vectorizer', TfidfTransformer(selected_tfidf)),\n",
    "        ('scaler', Scaler()),\n",
    "        ('feature_cleaner', Cleaner()),\n",
    "        ('classifier', SVC(kernel='linear', class_weight='balanced', random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(fs_test_pipeline, X_sample, y_sample)\n",
    "\n",
    "report = classification_report(\n",
    "    y_sample,\n",
    "    y_pred,\n",
    "    target_names=['happy', 'sad', 'angry', 'scared', 'neutral'],\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print('Previous number of TF-IDF features:', len(ds_vocabulary))\n",
    "print('Previous number of n-gram features:', len(important_ngrams))\n",
    "print('Selected number of TF-IDF features:', len(selected_tfidf))\n",
    "print('Selected number of n-gram features:', len(selected_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- The pipeline maintained similar performance with less than half of its original features\n",
    "- I'll be using these features from now on during preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "**Strategy**\n",
    "1. Experiment by tuning 9 times, each run with a wide range of hyperparameters, 3 folds, and 5 iterations to quickly narrow down optimal hyperparameter ranges using RandomizedSearchCV\n",
    "2. Use the most promising hyperparameters to tune the model by 5 folds to find highly optimal hyperparameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Experiment\n",
    "\n",
    "**Goal**: Narrow down optimal hyperparameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define specific parameter grid for each kernel\n",
    "param_grids = [\n",
    "    {\n",
    "        'C': np.logspace(-6, 6, 13),\n",
    "        'kernel': ['poly'],\n",
    "        'degree': [2, 3, 4, 5],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'coef0': np.linspace(-1, 1, 5),\n",
    "        'tol': np.logspace(-5, -1, 5),\n",
    "        'max_iter': [1000, 10000, 100000],\n",
    "        'shrinking': [True, False],\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-6, 6, 13),\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'tol': np.logspace(-5, -1, 5),\n",
    "        'max_iter': [1000, 10000, 100000],\n",
    "        'shrinking': [True, False],\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-6, 6, 13),\n",
    "        'kernel': ['linear'],\n",
    "        'tol': np.logspace(-5, -1, 5),\n",
    "        'max_iter': [1000, 10000, 100000],\n",
    "        'shrinking': [True, False],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Preprocess data\n",
    "preprocessors = Pipeline(\n",
    "    [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer(special_tokens)),\n",
    "        ('feature_engineer', FeatureEngineer(selected_ngrams)),\n",
    "        ('tfidf_vectorizer', TfidfTransformer(selected_tfidf)),\n",
    "        ('scaler', Scaler()),\n",
    "        ('feature_cleaner', Cleaner()),\n",
    "    ]\n",
    ")\n",
    "X_sample_preprocessed = preprocessors.fit_transform(X_sample)\n",
    "\n",
    "# Start experiment\n",
    "tuning_experiment = Experiment(\n",
    "    'narrow_hyperparameters', X_sample_preprocessed, y_sample\n",
    ")\n",
    "\n",
    "# Random search over param_grids\n",
    "for i in range(3):\n",
    "    for grid in param_grids:\n",
    "        tuning_experiment.run_tuning(\n",
    "            'SVM',\n",
    "            RandomizedSearchCV(\n",
    "                SVC(class_weight='balanced', probability=True, random_state=0),\n",
    "                param_distributions=grid,\n",
    "                scoring='f1_macro',\n",
    "                n_iter=5,\n",
    "                cv=3,\n",
    "                verbose=0,\n",
    "                n_jobs=-1,\n",
    "                random_state=i,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "![results](https://i.imgur.com/d1AehKH.png)\n",
    "- Best hyperparameters (values with multiple occurrences):\n",
    "\t- C: [0.1, 1, 10, 100, 1000]\n",
    "\t- coef0: [1]\n",
    "\t- kernel: ['rbf', 'linear']\n",
    "\t- tol: [0.1, 0.01, 0.001, 0.00001]\n",
    "\t- shrinking: [True, False]\n",
    "\t- max_iter: [100000, 10000]\n",
    "\t- gamma: ['unknown', 'scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "\n",
    "Tune and evaluate hyperparameters narrowed down by the previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'rbf': {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale'],\n",
    "        'tol': [0.1, 0.01, 0.001, 0.00001],\n",
    "        'max_iter': [10000, 100000],\n",
    "        'shrinking': [True, False]\n",
    "    },\n",
    "    'linear': {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['linear'],\n",
    "        'tol': [0.1, 0.01, 0.001, 0.00001],\n",
    "        'max_iter': [10000, 100000],\n",
    "        'shrinking': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "for kernel, grid in param_grids.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        SVC(class_weight='balanced', probability=True, random_state=0),\n",
    "        param_grid=grid,\n",
    "        scoring='f1_macro',\n",
    "        verbose=0, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_sample_preprocessed, y_sample)\n",
    "\n",
    "    y_pred = cross_val_predict(\n",
    "        grid_search.best_estimator_, X_sample_preprocessed, y_sample\n",
    "    )\n",
    "    report = classification_report(\n",
    "        y_sample, y_pred,\n",
    "        target_names=['happy', 'sad', 'angry', 'scared', 'neutral'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print('grid_search', kernel, 'hyperparameters:', grid_search.best_params_)\n",
    "    print('grid_search', kernel, 'report:')\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'C': 1,\n",
    "    'kernel': 'linear',\n",
    "    'max_iter': 100000,\n",
    "    'shrinking': True,\n",
    "    'tol': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[['text']]\n",
    "y = ds['emotion'].map({'happy': 0, 'sad': 1, 'angry': 2, 'scared': 3, 'neutral': 4})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "best_pipeline = Pipeline(\n",
    "    [\n",
    "        ('reddit_text_cleaner', RedditTextCleaner()),\n",
    "        ('tokenizer', Tokenizer(special_tokens)),\n",
    "        ('feature_engineer', FeatureEngineer(selected_ngrams)),\n",
    "        ('tfidf_vectorizer', TfidfTransformer(selected_tfidf)),\n",
    "        ('scaler', Scaler()),\n",
    "        ('feature_cleaner', Cleaner()),\n",
    "        ('classifier', SVC(class_weight='balanced', random_state=0, **best_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=['happy', 'sad', 'angry', 'scared', 'neutral'],\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "pipeline_name = 'example-pipeline'\n",
    "gc_project_id = 'example-project'\n",
    "gc_bucket_id = 'example-bucket'\n",
    "\n",
    "# Pickle and save model locally\n",
    "dumped_pipeline_name = f'{pipeline_name}.joblib'\n",
    "joblib.dump(best_pipeline, dumped_pipeline_name)\n",
    "\n",
    "# Compress the model into a tar.gz file\n",
    "compressed_pipeline_name = f'{pipeline_name}.tar.gz'\n",
    "with tarfile.open(compressed_pipeline_name, 'w:gz') as tar:\n",
    "    tar.add(dumped_pipeline_name, arcname=dumped_pipeline_name)\n",
    "\n",
    "# Connect to GCS\n",
    "storage_client = storage.Client(project=gc_project_id)\n",
    "bucket = storage_client.bucket(gc_bucket_id)\n",
    "\n",
    "# Upload compressed model\n",
    "blob = bucket.blob(compressed_pipeline_name)\n",
    "blob.upload_from_filename(compressed_pipeline_name)\n",
    "\n",
    "print(f'Model uploaded to gs://{gc_project_id}/{compressed_pipeline_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
